# -*- coding: utf-8 -*-
"""Sentiment Analysis for Hospital Patient Reviews V1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yI3ywwFNE2_MF1CjBIpG33bFWy4z9tP5
"""

import numpy as np
import pandas as pd
import re
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
import matplotlib.pyplot as plt
import seaborn as sns
import itertools
import collections
import matplotlib

#To print mulitple statements in single command
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity="all"

"""***Import and Visualize Data***"""

df = pd.read_excel('patient_reviews.xlsx')
df.info()
df.shape

df.head()

# prompt: Using dataframe df: split data in the Sentiment column

import pandas as pd
import ast

# Convert the string representation of dictionaries to actual dictionaries
df['Sentiment'] = df['Sentiment'].apply(lambda x: ast.literal_eval(x))

# Create new columns from the dictionary values
df['neg'] = df['Sentiment'].apply(lambda x: x['neg'])
df['neu'] = df['Sentiment'].apply(lambda x: x['neu'])
df['pos'] = df['Sentiment'].apply(lambda x: x['pos'])
df['compound'] = df['Sentiment'].apply(lambda x: x['compound'])

# Drop the original Sentiment column
df = df.drop('Sentiment', axis=1)

df.head()

!pip install openpyxl
# or
!pip install xlsxwriter

import pandas as pd


# Save to Excel file
df.to_excel('dataframe.xlsx', index=False, engine='openpyxl')  # or 'xlsxwriter'

from google.colab import files

# Download the file
files.download('dataframe.xlsx')

data = df['Review Text'].to_frame()
data.head()  # Display the first few rows of the new DataFrame

"""***Text Preprocessing***"""

import nltk
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from textblob import TextBlob

# Spell correct using text blob for the first 1000 records
from textblob import TextBlob
dfs = data['Review Text'][:5].apply(lambda x: str(TextBlob(x).correct()))

dfs

corpus = [] # form an empty list called corpus
ps = PorterStemmer() # for stemming the words to original form, for example - "running to run"
stop_words = set(stopwords.words('english')) # remove words such as will, I, and, at etc.
for i in range(0, 1362): # iterate through the list of all 1362 comments
  # remove handles (@), numbers, urls, emojis and any other special characters to have only text
  data_clean = re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|[0-9]", '', str(data["Review Text"][i]))
  # convert all words to lower case
  data_clean = data_clean.lower()
  # split sentences into words
  data_clean = data_clean.split()
  # remove English stop words
  data_clean = [x for x in data_clean if not x in stop_words]
  # stem words to the base form
  data_clean = [ps.stem(x) for x in data_clean]
  # join words to form the original sentences
  data_clean = " ".join(data_clean)
  # append to the list to get all Comments in one place
  corpus.append(data_clean)
print(corpus)

def comment_to_words(Review):
    letters_only = re.sub("[^a-zA-Z]", " ",Reviews)
    words = letters_only.lower().split()
    stops = set(stopwords.words("english"))
    meaningful_words = [w for w in words if not w in stops]
    return( " ".join( meaningful_words ))

"""# TF-IDF Vectorizor"""

data['Reviews'] = data['Review Text'].apply(lambda x : ' '.join([w for w in str(x).split() if len(w)>3]))

tokenized_comments = data['Review Text'].apply(lambda x: str(x).split()) # tokenizing

tokenized_comments.head()

from nltk.stem.porter import *
stemmer = PorterStemmer()

tokenized_comments = tokenized_comments.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming

for i in range(len(tokenized_comments)):
    tokenized_comments[i] = ' '.join(tokenized_comments[i])

data['Review Text'] = tokenized_comments

#Transform the data
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(data['Review Text'])
print(X)

data.head()

#Build the clusters
true_k = 3
model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)
model.fit(X)

#Profile the clusters
print("Top terms per cluster:")
order_centroids = model.cluster_centers_.argsort()[:, ::-1]
terms = vectorizer.get_feature_names_out()
for i in range(true_k):
    print("Cluster %d:" % i),
    for ind in order_centroids[i, :10]:
        print(' %s' % terms[ind]),
    print

"""# Bag of Words"""

bagofwords=[]
for i in range (0,1362):
    words = corpus[i].split()
    bagofwords.append(words)

bagofwords[1361]

import itertools
import collections
all_words=list(itertools.chain(*bagofwords))
print(all_words[0:9546])
print()
print('Most commonly occuring words and frequency\n')
words_freq=collections.Counter(all_words)
words_freq.most_common(20)

"""# Word Frequency"""

freq_word_df = pd.DataFrame(words_freq.most_common(20),columns=['Words','Frequency'])
freq_word_df.head()

freq_word_df.to_csv("/content/word_frequency.csv")

fig,ax=plt.subplots(figsize=(25,10))
freq_word_df.sort_values(by='Frequency').plot.barh(x='Words',y='Frequency',ax=ax,color='orange')
plt.show();

from wordcloud import WordCloud,STOPWORDS
import matplotlib.pyplot as plt
word_string=' '.join(freq_word_df.Words)
wordcloud=WordCloud(#stopwords=STOPWORDS,
                       background_color='black',
                     max_words=20
                      ).generate(word_string)

plt.figure(figsize=(15,10))
plt.clf()
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

"""# Word2Vec"""

import re
import nltk
nltk.download('punkt')
nltk.download('stopwords')

from gensim.models import Word2Vec
word2vec = Word2Vec(bagofwords, min_count=2)

# Instead of using word2vec.wv.vocab, use:
vocabulary = word2vec.wv.key_to_index
print(vocabulary)

v1 = word2vec.wv['dr']

sim_words = word2vec.wv.most_similar('dr')

sim_words

